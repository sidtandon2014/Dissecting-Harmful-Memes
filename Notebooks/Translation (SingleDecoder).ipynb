{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (61.2.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: jinja2 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\miniconda\\envs\\nlp\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import random\n",
    "\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../Dataset/train.parquet\")\n",
    "valid = pd.read_parquet(\"../Dataset/valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[BOTH] bernie sanders [SEP] elizabeth warren [EOTH]',\n",
       "       '[BVIL] uk government [EVIL]',\n",
       "       '[BH] thais [EH]  [BOTH] hong kong [EOTH]', ...,\n",
       "       '[BOTH] msnbc [SEP] bernie sanders [SEP] democratic party [SEP] joe biden [SEP] democratic debate [EOTH]',\n",
       "       '[BOTH] barack obama [EOTH]',\n",
       "       '[BOTH] biden obama meme [SEP] john robinson [SEP] memes [SEP] joe biden [SEP] barack obama [EOTH]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "special_tokens = [\"<pad>\",\"<bos>\",\"<eos>\"]\n",
    "special_target_tokens = special_tokens + [\"[EH]\",\"[BH]\",\"[EVIC]\",\"[EVIC]\",\"[BVIL]\",\"[EVIL]\",\"[BOTH]\",\"[EOTH]\"]\n",
    "\n",
    "def get_training_corpus(col):\n",
    "    dataset = train.copy()\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[col]\n",
    "\n",
    "checkpoint  = 'bert-base-uncased' #\"cardiffnlp/twitter-roberta-base\"\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "src_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(\"OCR\"), 52000, new_special_tokens =special_tokens)\n",
    "trgt_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(\"Target\"), 52000, new_special_tokens = special_target_tokens)\n",
    "#tokenizer.encode(\"How is life braack obama\", return_tensors = \"pt\", add_special_tokens = False)\n",
    "#src_tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "#trgt_tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6) <bos>\n",
      "tensor(8) [EH]\n",
      "tensor(3057) how\n",
      "tensor(442) is\n",
      "tensor(1389) life\n",
      "tensor(161) barack\n",
      "tensor(156) obama\n",
      "tensor(1307) abc\n",
      "tensor(3273) ##dd\n",
      "tensor(65) ##e\n",
      "tensor(257) wuhan\n",
      "tensor(226) china\n",
      "tensor(247) 2020\n",
      "tensor(9) [BH]\n"
     ]
    }
   ],
   "source": [
    "def verify_tokens(_tokenizer):\n",
    "    tensor = _tokenizer.encode(\"<bos> [EH] How is life barack obama abcdde wuhan china 2020 [BH]\", return_tensors = \"pt\", add_special_tokens = False)\n",
    "    for _tensor in tensor.view(-1):\n",
    "        print(_tensor, _tokenizer.decode(_tensor))\n",
    "    \n",
    "verify_tokens(trgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25957, 7272)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_tokenizer.vocab), len(trgt_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = src_tokenizer.vocab\n",
    "trgt_vocab = trgt_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "s_PAD_IDX =src_vocab['<pad>']\n",
    "s_BOS_IDX = src_vocab['<bos>']\n",
    "s_EOS_IDX = src_vocab['<eos>']\n",
    "\n",
    "t_PAD_IDX =trgt_vocab['<pad>']\n",
    "t_BOS_IDX = trgt_vocab['<bos>']\n",
    "t_EOS_IDX = trgt_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  src_batch, trgt_batch = [], []\n",
    "  for (src_item, trgt_item) in data_batch:\n",
    "    src_batch.append(torch.cat([torch.tensor([s_BOS_IDX])\n",
    "        , src_tokenizer.encode(src_item, return_tensors = \"pt\", add_special_tokens = False).view(-1)\n",
    "        , torch.tensor([s_EOS_IDX])], dim=0).type(torch.LongTensor))\n",
    "    \n",
    "    trgt_batch.append(torch.cat([torch.tensor([t_BOS_IDX])\n",
    "        ,trgt_tokenizer.encode(\",\".join(list(trgt_item)), return_tensors = \"pt\", add_special_tokens = False).view(-1) \n",
    "        , torch.tensor([t_EOS_IDX])], dim=0).type(torch.LongTensor))\n",
    "  src_batch = pad_sequence(src_batch, padding_value=s_PAD_IDX)\n",
    "  trgt_batch = pad_sequence(trgt_batch, padding_value=t_PAD_IDX)\n",
    "  return src_batch, trgt_batch\n",
    "\n",
    "train_data = list(zip(train.OCR.values, train.All_Entities.values))\n",
    "valid_data = list(zip(valid.OCR.values, valid.All_Entities.values))\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "#test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   6,    6,    6,  ...,    6,    6,    6],\n",
       "         [ 337,  683, 3399,  ..., 1413,  795,  397],\n",
       "         [6045, 1766,  991,  ...,  307, 2345, 3718],\n",
       "         ...,\n",
       "         [   5,    5,    5,  ...,    5,    5,    5],\n",
       "         [   5,    5,    5,  ...,    5,    5,    5],\n",
       "         [   5,    5,    5,  ...,    5,    5,    5]]),\n",
       " tensor([[   6,    6,    6,  ...,    6,    6,    6],\n",
       "         [ 161,  128,    7,  ...,  128,  313, 1579],\n",
       "         [ 156,  126,    5,  ...,  126,  792,    7],\n",
       "         ...,\n",
       "         [   5,    5,    5,  ...,    5,    5,    5],\n",
       "         [   5,    5,    5,  ...,    5,    5,    5],\n",
       "         [   5,    5,    5,  ...,    5,    5,    5]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,436,904 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, dropout = dropout, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, src, start_token, end_token, max_len): \n",
    "        batch_size = src.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        out = []\n",
    "        current_token = start_token\n",
    "        i = 0\n",
    "        while (current_token.item() != end_token.item()) and (i<max_len):\n",
    "            output, hidden = self.decoder(current_token, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1) \n",
    "            current_token = top1\n",
    "            out.append(top1.item())\n",
    "            i = i+1\n",
    "        return out\n",
    "\n",
    "\n",
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(trgt_vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 50\n",
    "DEC_EMB_DIM = 50\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 128\n",
    "ATTN_DIM = 32\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_uniform_(param.data, gain=nn.init.calculate_gain('relu'))\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25957 7272\n"
     ]
    }
   ],
   "source": [
    "print(INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: when scoring the performance of a language translation model in particular, \n",
    "# we have to tell the nn.CrossEntropyLoss function to ignore the indices where the target is simply padding.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=t_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 15s\n",
      "\tTrain Loss: 8.753 | Train PPL: 6330.111\n",
      "\t Val. Loss: 8.306 |  Val. PPL: 4048.907\n",
      "Epoch: 02 | Time: 0m 14s\n",
      "\tTrain Loss: 6.365 | Train PPL: 580.958\n",
      "\t Val. Loss: 5.529 |  Val. PPL: 252.008\n",
      "Epoch: 03 | Time: 0m 14s\n",
      "\tTrain Loss: 4.782 | Train PPL: 119.340\n",
      "\t Val. Loss: 5.476 |  Val. PPL: 238.962\n",
      "Epoch: 04 | Time: 0m 14s\n",
      "\tTrain Loss: 4.716 | Train PPL: 111.666\n",
      "\t Val. Loss: 5.500 |  Val. PPL: 244.704\n",
      "Epoch: 05 | Time: 0m 14s\n",
      "\tTrain Loss: 4.690 | Train PPL: 108.901\n",
      "\t Val. Loss: 5.629 |  Val. PPL: 278.248\n",
      "Epoch: 06 | Time: 0m 14s\n",
      "\tTrain Loss: 4.666 | Train PPL: 106.303\n",
      "\t Val. Loss: 5.576 |  Val. PPL: 264.012\n",
      "Epoch: 07 | Time: 0m 14s\n",
      "\tTrain Loss: 4.643 | Train PPL: 103.842\n",
      "\t Val. Loss: 5.557 |  Val. PPL: 259.090\n",
      "Epoch: 08 | Time: 0m 14s\n",
      "\tTrain Loss: 4.617 | Train PPL: 101.174\n",
      "\t Val. Loss: 5.558 |  Val. PPL: 259.356\n",
      "Epoch: 09 | Time: 0m 14s\n",
      "\tTrain Loss: 4.591 | Train PPL:  98.570\n",
      "\t Val. Loss: 5.587 |  Val. PPL: 266.911\n",
      "Epoch: 10 | Time: 0m 14s\n",
      "\tTrain Loss: 4.563 | Train PPL:  95.908\n",
      "\t Val. Loss: 5.642 |  Val. PPL: 282.028\n",
      "Epoch: 11 | Time: 0m 14s\n",
      "\tTrain Loss: 4.530 | Train PPL:  92.766\n",
      "\t Val. Loss: 5.669 |  Val. PPL: 289.659\n",
      "Epoch: 12 | Time: 0m 14s\n",
      "\tTrain Loss: 4.513 | Train PPL:  91.214\n",
      "\t Val. Loss: 5.621 |  Val. PPL: 276.153\n",
      "Epoch: 13 | Time: 0m 14s\n",
      "\tTrain Loss: 4.476 | Train PPL:  87.921\n",
      "\t Val. Loss: 5.677 |  Val. PPL: 292.196\n",
      "Epoch: 14 | Time: 0m 14s\n",
      "\tTrain Loss: 4.456 | Train PPL:  86.176\n",
      "\t Val. Loss: 5.674 |  Val. PPL: 291.163\n",
      "Epoch: 15 | Time: 0m 14s\n",
      "\tTrain Loss: 4.423 | Train PPL:  83.342\n",
      "\t Val. Loss: 5.824 |  Val. PPL: 338.244\n",
      "Epoch: 16 | Time: 0m 14s\n",
      "\tTrain Loss: 4.401 | Train PPL:  81.516\n",
      "\t Val. Loss: 5.621 |  Val. PPL: 276.138\n",
      "Epoch: 17 | Time: 0m 14s\n",
      "\tTrain Loss: 4.367 | Train PPL:  78.800\n",
      "\t Val. Loss: 5.747 |  Val. PPL: 313.284\n",
      "Epoch: 18 | Time: 0m 14s\n",
      "\tTrain Loss: 4.326 | Train PPL:  75.672\n",
      "\t Val. Loss: 5.714 |  Val. PPL: 303.084\n",
      "Epoch: 19 | Time: 0m 14s\n",
      "\tTrain Loss: 4.322 | Train PPL:  75.333\n",
      "\t Val. Loss: 5.788 |  Val. PPL: 326.426\n",
      "Epoch: 20 | Time: 0m 14s\n",
      "\tTrain Loss: 4.293 | Train PPL:  73.149\n",
      "\t Val. Loss: 5.932 |  Val. PPL: 376.814\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def trainmodel(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = trainmodel(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "#test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "#print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../Models/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../Models/model.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, input_sentence, max_len):\n",
    "    start_token = src_vocab['<bos>']\n",
    "    end_token = src_vocab['<eos>']\n",
    "    start_token = torch.LongTensor([start_token]).to(device)\n",
    "    end_token = torch.LongTensor([end_token]).to(device)\n",
    "    \n",
    "    sent_tokens = torch.LongTensor( src_tokenizer.encode(input_sentence, return_tensors = \"pt\", add_special_tokens = False).view(-1, 1)).to(device)\n",
    "    \n",
    "    #Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(sent_tokens, start_token, end_token, max_len)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dr grayfang,quarantine,ice cream'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(list(valid.loc[5][\"All_Entities\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHO WOULD WIN? Thanos One hantavirus boy Sorry spoiler guys \n",
      "hantavirus,thanos,hanta virus,thanos vs hanta virus\n",
      "Output Tokens : [128, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Generated : donald [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "input_sentence = valid.loc[index][\"OCR\"]\n",
    "\n",
    "output_sentence = \",\".join(list(valid.loc[index][\"All_Entities\"]))\n",
    "print(input_sentence)\n",
    "print(output_sentence)\n",
    "\n",
    "max_len = 35\n",
    "out = translate(model, input_sentence, max_len)\n",
    "print(\"Output Tokens : {}\".format(out))\n",
    "generated_text = trgt_tokenizer.decode(out)\n",
    "print(\"Generated : {}\".format(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(613) dr\n",
      "tensor(6861) grayfang\n",
      "tensor(2883) dor\n",
      "tensor(278) ##or\n",
      "tensor(4521) ##ayfang\n",
      "tensor(47) d\n",
      "tensor(17395) ##pra\n",
      "tensor(12704) ##yt\n",
      "tensor(724) ##ang\n",
      "tensor(20) -\n",
      "tensor(18948) folon\n",
      "tensor(282) ##ing\n",
      "tensor(11955) oy\n",
      "tensor(6472) ##tan\n",
      "tensor(210) ##g\n",
      "tensor(9) \"\n",
      "tensor(288) the\n",
      "tensor(891) best\n",
      "tensor(1033) thing\n",
      "tensor(651) quarantine\n",
      "tensor(2471) ice\n",
      "tensor(3388) cream\n",
      "tensor(9) \"\n",
      "tensor(1535) ge\n",
      "tensor(809) ve\n",
      "tensor(499) ##ve\n",
      "tensor(1552) seen\n",
      "tensor(6586) thee\n",
      "tensor(4669) ##eb\n",
      "tensor(205) ##l\n",
      "tensor(849) ##ble\n",
      "tensor(3079) yo\n",
      "tensor(201) ##m\n",
      "tensor(3508) ##no\n",
      "tensor(588) so\n",
      "tensor(199) ##e\n",
      "tensor(48) e\n",
      "tensor(1944) cour\n",
      "tensor(199) ##e\n",
      "tensor(48) e\n",
      "tensor(411) con\n",
      "tensor(4190) ##mo\n",
      "tensor(1069) ##lem\n",
      "tensor(1431) ##oe\n",
      "tensor(5291) por\n",
      "tensor(4605) ld\n",
      "tensor(4092) ae\n",
      "tensor(274) ##an\n"
     ]
    }
   ],
   "source": [
    "def verify_tokens(_tokenizer, sentence):\n",
    "    tensor = _tokenizer.encode(sentence, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    for _tensor in tensor.view(-1):\n",
    "        print(_tensor, _tokenizer.decode(_tensor))\n",
    "    \n",
    "verify_tokens(src_tokenizer, valid.loc[5][\"OCR\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf56965869f9128303720e02ce7fd8290252d113f24b711c0c2cea593c860638"
  },
  "kernelspec": {
   "display_name": "nlp-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
