{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(ds, tokenizer):\n",
    "  counter_src = {}\n",
    "  counter_trgt = {}\n",
    "  specials={'<unk>':0, '<pad>':1, '<bos>':2, '<eos>':3}\n",
    "  index_src = 4\n",
    "  index_trgt = 4\n",
    "  counter_src = specials\n",
    "  counter_trgt = specials\n",
    "\n",
    "  for _, row in ds.iterrows():\n",
    "    for _token in tokenizer(row.OCR):\n",
    "      if _token not in counter_src:\n",
    "        counter_src[_token] = index_src\n",
    "        index_src+=1\n",
    "\n",
    "    for _token in tokenizer(row.Target):\n",
    "      if _token not in counter_trgt:\n",
    "        counter_trgt[_token] = index_trgt\n",
    "        index_trgt+=1\n",
    "\n",
    "  return Vocab(counter_src), Vocab(counter_trgt)\n",
    "\n",
    "src_vocab, trgt_vocab = build_vocab(train, tokenizer)\n",
    "\n",
    "#test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(ds,tokenizer):\n",
    "  data = []\n",
    "  for _, row in ds.iterrows():\n",
    "    src_tensor_ = torch.tensor([src_vocab[token] if token in src_vocab else src_vocab[\"<unk>\"] for token in tokenizer(row.OCR)],dtype=torch.long)\n",
    "    trgt_tensor_ = torch.tensor([trgt_vocab[token] if token in trgt_vocab else trgt_vocab[\"<unk>\"] for token in tokenizer(row.Target) ],\n",
    "                            dtype=torch.long)\n",
    "    data.append((src_tensor_, trgt_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train, tokenizer)\n",
    "val_data =  data_process(valid, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
